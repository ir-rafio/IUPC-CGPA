{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXVIs4Jn6LtB"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import regex as re\n",
        "import json\n",
        "import os\n",
        "from datetime import date\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define Team Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af2yLZmq-YIk"
      },
      "outputs": [],
      "source": [
        "class Team:\n",
        "    def __init__(self, rank, name, institution, solved, penalty, first_solve_count):\n",
        "        self.rank = rank\n",
        "        self.name = name\n",
        "        self.institution = institution.upper()\n",
        "        self.solved = solved\n",
        "        self.penalty = penalty\n",
        "        self.first_solve_count = first_solve_count\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        if not isinstance(other, Team): return NotImplemented\n",
        "        return self.rank < other.rank\n",
        "\n",
        "    def __repr__(self):\n",
        "        return json.dumps(self.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define ContestParser Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnLgs6ytFJte"
      },
      "outputs": [],
      "source": [
        "class ContestParser(ABC):\n",
        "    _parsers = {}\n",
        "\n",
        "    def parse(self, filepaths):\n",
        "        team_list = []\n",
        "        for filepath in filepaths:\n",
        "            team_list.extend(self.parseFile(filepath))\n",
        "        return team_list\n",
        "\n",
        "    @abstractmethod\n",
        "    def parseFile(self, filepath):\n",
        "        \"\"\"\n",
        "        Parses a single HTML file containing contest standings and returns a list of Team objects.\n",
        "\n",
        "        Args:\n",
        "            filepath (str): Path to the HTML file containing the contest data.\n",
        "\n",
        "        Returns:\n",
        "            list[Team]: A list of Team objects.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @classmethod\n",
        "    def get_parser(cls, key):\n",
        "        \"\"\"\n",
        "        Returns a parser instance for the given key.\n",
        "\n",
        "        Args:\n",
        "            key (str): The key identifying the parser.\n",
        "\n",
        "        Returns:\n",
        "            ContestParser: The parser instance corresponding to the key.\n",
        "        \"\"\"\n",
        "        key = key.upper()\n",
        "        if key not in cls._parsers: raise ValueError(f\"No parser found for key: {key}\")\n",
        "\n",
        "        return cls._parsers[key]\n",
        "\n",
        "    @classmethod\n",
        "    def register_parser(cls, key, parser_instance):\n",
        "        \"\"\"\n",
        "        Registers a parser instance with the specified key.\n",
        "\n",
        "        Args:\n",
        "            key (str): The key to associate with the parser.\n",
        "            parser_instance (ContestParser): The parser instance to register.\n",
        "        \"\"\"\n",
        "        if key.upper() in cls._parsers: raise ValueError(f\"Parser for key '{key}' is already registered.\")\n",
        "\n",
        "        cls._parsers[key.upper()] = parser_instance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Implementations of ContestParser Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJOS5uLpKOAv"
      },
      "outputs": [],
      "source": [
        "class TophParser(ContestParser):\n",
        "    def parseFile(self, filepath):\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "            contest_html = file.read()\n",
        "\n",
        "        soup = BeautifulSoup(contest_html, \"html.parser\")\n",
        "        table = soup.find(\"table\")\n",
        "        if not table: raise ValueError(\"No table found in the HTML document.\")\n",
        "\n",
        "        rows = table.find_all(\"tr\")\n",
        "        team_list = []\n",
        "\n",
        "        for row in rows[1:]:\n",
        "            cells = row.find_all(\"td\")\n",
        "            if len(cells) < 3: continue\n",
        "\n",
        "            try:\n",
        "                rank = int(cells[0].get_text(strip=True))\n",
        "\n",
        "                team_name = cells[1].contents[0].strip()\n",
        "                institution_div = cells[1].find(\"div\", class_=\"adjunct\")\n",
        "                institution = institution_div.get_text(strip=True) if institution_div else \"\"\n",
        "\n",
        "                solve_count = int(cells[2].find(\"strong\").get_text(strip=True))\n",
        "                penalty_text = cells[2].find(\"div\", class_=\"adjunct\").get(\"data-tippy-content\")\n",
        "                penalty = int(re.search(r\"Penalty: (\\d+)\", penalty_text).group(1))\n",
        "\n",
        "                first_solve_count = sum(\n",
        "                    1 for cell in cells[3:]\n",
        "                    if cell.find(\"img\", class_=\"icon green\") and\n",
        "                    cell.find(\"img\", class_=\"icon green\").get(\"data-tippy-content\") == \"First to Solve\"\n",
        "                )\n",
        "\n",
        "                team = Team(\n",
        "                    name=team_name,\n",
        "                    institution=institution,\n",
        "                    rank=rank,\n",
        "                    solved=solve_count,\n",
        "                    penalty=penalty,\n",
        "                    first_solve_count=first_solve_count,\n",
        "                )\n",
        "                team_list.append(team)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing row: {row}\")\n",
        "                print(e)\n",
        "\n",
        "        return team_list\n",
        "\n",
        "ContestParser.register_parser(\"toph\", TophParser())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6m3kFnfFPXH"
      },
      "outputs": [],
      "source": [
        "class BAPSparser(ContestParser):\n",
        "    def parseFile(self, filepath):\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "            contest_html = file.read()\n",
        "\n",
        "        soup = BeautifulSoup(contest_html, \"html.parser\")\n",
        "        table = soup.find(\"table\")\n",
        "        if not table: raise ValueError(\"No table found in the HTML document.\")\n",
        "\n",
        "        rows = table.find_all(\"tr\")\n",
        "        team_list = []\n",
        "\n",
        "        for row in rows[1:]:\n",
        "            cells = row.find_all(\"td\")\n",
        "            if len(cells) < 3: continue\n",
        "\n",
        "            try:\n",
        "                rank = int(cells[0].get_text(strip=True))\n",
        "                team_name = cells[1].find(\"strong\").get_text(strip=True)\n",
        "                institution_div = cells[1].find(\"div\")\n",
        "                institution = institution_div.get_text(strip=True) if institution_div else \"\"\n",
        "                solve_count_text = cells[2].get_text(strip=True)\n",
        "                solve_count = int(re.search(r\"(\\d+)\", solve_count_text).group(1))\n",
        "                penalty = int(re.search(r\"\\((\\d+)\\)\", solve_count_text).group(1))\n",
        "                first_solve_count = sum(\n",
        "                    1 for cell in cells[3:]\n",
        "                    if cell.find(\"div\", style=re.compile(r\"animation:.*shine.*\"))\n",
        "                )\n",
        "\n",
        "                team = Team(\n",
        "                    name=team_name,\n",
        "                    institution=institution,\n",
        "                    rank=rank,\n",
        "                    solved=solve_count,\n",
        "                    penalty=penalty,\n",
        "                    first_solve_count=first_solve_count,\n",
        "                )\n",
        "                team_list.append(team)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing row: {row}\")\n",
        "                print(e)\n",
        "\n",
        "        return team_list\n",
        "\n",
        "ContestParser.register_parser(\"baps\", BAPSparser())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define Contest Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxqFNtuJKoP2"
      },
      "outputs": [],
      "source": [
        "class Contest:\n",
        "    def __init__(self, name, filepaths, parser, date_string):\n",
        "        if not isinstance(parser, ContestParser):\n",
        "            raise TypeError(\"parser must be an instance of ContestParser\")\n",
        "\n",
        "        self.name = name.upper()\n",
        "        self.team_list = parser.parse(filepaths)\n",
        "\n",
        "        self.max_solved = 0\n",
        "        self.institution_map = {}\n",
        "        self.date = date.fromisoformat(date_string)\n",
        "\n",
        "        for team in self.team_list:\n",
        "            self.max_solved = max(self.max_solved, team.solved)\n",
        "\n",
        "            if team.institution not in self.institution_map: self.institution_map[team.institution] = []\n",
        "            self.institution_map[team.institution].append(team)\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        if not isinstance(other, Contest): return NotImplemented\n",
        "        return self.date < other.date\n",
        "\n",
        "    def __repr__(self):\n",
        "        return json.dumps({\n",
        "            \"name\": self.name,\n",
        "            \"date\": self.date.isoformat(),\n",
        "            \"max_solved\": self.max_solved,\n",
        "            \"team_list\": [team.__dict__ for team in self.team_list],\n",
        "            \"institution_map\": {k: [team.__dict__ for team in v] for k, v in self.institution_map.items()}\n",
        "        }, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define Institution Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hB8bf32TT2tI"
      },
      "outputs": [],
      "source": [
        "class Institution:\n",
        "    def __init__(self, name, alt_names=[]):\n",
        "        self.name = name.upper()\n",
        "        self.alt_names = [alt_name.upper() for alt_name in alt_names]\n",
        "        self.contest_map = {}\n",
        "\n",
        "    def add_contest(self, contest):\n",
        "        name = self.name\n",
        "        alt_names = self.alt_names\n",
        "        institution_map = contest.institution_map\n",
        "        contest_name = contest.name\n",
        "\n",
        "        contest_team_list = []\n",
        "\n",
        "        if name in institution_map:\n",
        "            contest_team_list.extend(institution_map[name])\n",
        "\n",
        "        for alt_name in alt_names:\n",
        "            if alt_name in institution_map:\n",
        "                contest_team_list.extend(institution_map[alt_name])\n",
        "\n",
        "        if len(contest_team_list) > 0:\n",
        "            self.contest_map[contest_name] = sorted(contest_team_list)\n",
        "\n",
        "    def get_contest_teams(self, contest_name):\n",
        "        if contest_name in self.contest_map:\n",
        "            return self.contest_map[contest_name]\n",
        "        return None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return json.dumps({\n",
        "            \"name\": self.name,\n",
        "            \"alt_names\": self.alt_names,\n",
        "            \"contest_map\": {\n",
        "                contest_name: [team.__dict__ for team in teams]\n",
        "                for contest_name, teams in self.contest_map.items()\n",
        "            }\n",
        "        }, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNvP1wCb5wAC"
      },
      "outputs": [],
      "source": [
        "def load_contests_from_json(file_path):\n",
        "    contest_dir = \"./input/grading/contest_files/\"\n",
        "    contests = {}\n",
        "\n",
        "    with open(file_path, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "        for contest in data[\"contests\"]:\n",
        "            name = contest[\"name\"]\n",
        "            filepaths = [contest_dir + filename for filename in contest[\"filenames\"]]\n",
        "            parser = ContestParser.get_parser(contest[\"parser\"])\n",
        "            date_string = contest[\"date\"]\n",
        "            contests[name] = Contest(name, filepaths, parser, date_string)\n",
        "\n",
        "    return contests\n",
        "\n",
        "def load_institutions_from_json(file_path):\n",
        "    institutions = []\n",
        "\n",
        "    with open(file_path, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "        for institution_data in data[\"institutions\"]:\n",
        "            name = institution_data[\"name\"]\n",
        "            alt_names = institution_data.get(\"alt_names\", [])\n",
        "            institutions.append(Institution(name, alt_names))\n",
        "\n",
        "    return institutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbbVy9LBV5jq"
      },
      "outputs": [],
      "source": [
        "contests_file_path = \"./input/grading/contests.json\"\n",
        "contests = load_contests_from_json(contests_file_path)\n",
        "\n",
        "institutions_file_path = \"./input/grading/institutions.json\"\n",
        "institutions = load_institutions_from_json(institutions_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "contest_list = sorted(contests.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for institution in institutions:\n",
        "    for contest in contest_list:\n",
        "        institution.add_contest(contest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "credits_file_path = \"./input/grading/credits.json\"\n",
        "with open(credits_file_path, \"r\") as file: credits_map = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define Calculator Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iC7nNl8hcE2R"
      },
      "outputs": [],
      "source": [
        "class GradeCalculator:\n",
        "    def get_grade_point(self, institution, contest, contest_name):\n",
        "        contest_teams = institution.get_contest_teams(contest_name)\n",
        "        if not contest_teams: return None\n",
        "\n",
        "        best4 = sorted(contest_teams)[:4]\n",
        "        team_grades = []\n",
        "\n",
        "        rank_decay_rate = 0.02\n",
        "        for team in best4:\n",
        "            grade = 4000 * (1 - rank_decay_rate) ** (team.rank - 1) * (team.solved / contest.max_solved) ** (1/2)\n",
        "\n",
        "            team_grades.append(grade)\n",
        "\n",
        "        k = 3.14159\n",
        "        lk_norm = np.mean(np.array(team_grades) ** k) ** (1 / k)\n",
        "        return lk_norm\n",
        "    \n",
        "    def get_cgpa(self, institution_list, contest_list, credits_map):\n",
        "        contest_name_list = [c.name for c in reversed(contest_list)]\n",
        "        institution_names = [inst.name for inst in institution_list]\n",
        "\n",
        "        marksheet = pd.DataFrame(\n",
        "            index=institution_names,\n",
        "            columns=[contest_name + \" Point\" for contest_name in contest_name_list]\n",
        "        )\n",
        "        history_cgpa = pd.DataFrame(\n",
        "            index=institution_names,\n",
        "            columns=[\"Rating After \" + contest_name for contest_name in contest_name_list]\n",
        "        )\n",
        "\n",
        "        time_decay_rate = 0.25\n",
        "        time_decay_pulse = 91.3125  # 3 months\n",
        "\n",
        "        weights_map = {}\n",
        "        gp_l2 = {name: 0.0 for name in institution_names}\n",
        "        credit_l2 = {name: 0.0 for name in institution_names}\n",
        "\n",
        "        contests_in_processing = []\n",
        "        for contest in contest_list:\n",
        "            latest_date = contest.date\n",
        "            contests_in_processing.append(contest)\n",
        "\n",
        "            for contest_in_processing in contests_in_processing:\n",
        "                weight = credits_map[contest_in_processing.name]\n",
        "\n",
        "                period = (latest_date - contest_in_processing.date).days // time_decay_pulse\n",
        "                weight *= ((1 - time_decay_rate) ** period)\n",
        "                \n",
        "                weights_map[contest_in_processing.name] = weight\n",
        "\n",
        "            gp_l2 = {name: 0.0 for name in institution_names}\n",
        "            credit_l2 = {name: 0.0 for name in institution_names}\n",
        "\n",
        "            for contest_in_processing in contests_in_processing:\n",
        "                weight = weights_map[contest_in_processing.name]\n",
        "                contest_name = contest_in_processing.name\n",
        "\n",
        "                for institution in institution_list:\n",
        "                    gp = self.get_grade_point(institution, contest_in_processing, contest_name)\n",
        "                    if contest_in_processing == contest:\n",
        "                        marksheet.at[institution.name, contest_name + \" Point\"] = gp\n",
        "                    if gp is not None:\n",
        "                        gp_l2[institution.name] += (gp * weight) ** 2\n",
        "                        credit_l2[institution.name] += weight ** 2\n",
        "\n",
        "            for name in institution_names:\n",
        "                if credit_l2[name] > 0:\n",
        "                    cgpa = (gp_l2[name] / credit_l2[name]) ** 0.5\n",
        "                else:\n",
        "                    cgpa = None\n",
        "                history_cgpa.at[name, \"Rating After \" + contest.name] = cgpa\n",
        "\n",
        "        last_col = history_cgpa.columns[0]\n",
        "        marksheet.insert(0, \"Rating\", history_cgpa[last_col])\n",
        "\n",
        "        marksheet = marksheet.infer_objects(copy=False)\n",
        "        history_cgpa = history_cgpa.infer_objects(copy=False)\n",
        "\n",
        "        marksheet.sort_values(by=\"Rating\", ascending=False, inplace=True)\n",
        "        history_cgpa = history_cgpa.loc[marksheet.index]\n",
        "\n",
        "        history_rank = history_cgpa.rank(axis=0, ascending=False, method='min')\n",
        "        history_rank.columns = [\"Rank After \" + c.split(\"Rating After \")[-1] for c in history_cgpa.columns]\n",
        "        \n",
        "        marksheet = marksheet.round(0).astype('Int64')\n",
        "        history_cgpa = history_cgpa.round(0).astype('Int64')\n",
        "        history_rank = history_rank.astype('Int64')\n",
        "\n",
        "        return marksheet, history_cgpa, history_rank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extra Calculators (Unused)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ELOcalculator:\n",
        "    def __init__(self, initial_rating=1500, k=32):\n",
        "        self.initial_rating = initial_rating\n",
        "        self.k = k\n",
        "\n",
        "    def get_performance(self, institution, contest, contest_name):\n",
        "        contest_teams = institution.get_contest_teams(contest_name)\n",
        "        if not contest_teams:\n",
        "            return None\n",
        "\n",
        "        best4 = sorted(contest_teams)[:4]\n",
        "        team_grades = []\n",
        "\n",
        "        rank_decay_rate = 0.02\n",
        "        for team in best4:\n",
        "            grade = 4000 * (1 - rank_decay_rate) ** (team.rank - 1) * (team.solved / contest.max_solved) ** 0.5\n",
        "            team_grades.append(grade)\n",
        "\n",
        "        k_norm = 3.14159\n",
        "        return (np.mean(np.array(team_grades) ** k_norm)) ** (1 / k_norm)\n",
        "\n",
        "    def get_expected_performance(self, inst_rating, others_ratings):\n",
        "        expected = 0.0\n",
        "        for other_rating in others_ratings:\n",
        "            expected += 1 / (1 + 10 ** ((other_rating - inst_rating) / 400))\n",
        "        return expected / len(others_ratings) if others_ratings else 0.0\n",
        "\n",
        "    def get_elo_rating(self, institution_list, contest_list, credits_map):\n",
        "        contest_name_list = [c.name for c in reversed(contest_list)]\n",
        "        institution_names = [inst.name for inst in institution_list]\n",
        "\n",
        "        ratings = {name: self.initial_rating for name in institution_names}\n",
        "        marksheet = pd.DataFrame(index=institution_names,\n",
        "                                 columns=[contest_name + \" Point\" for contest_name in contest_name_list])\n",
        "        history_elo = pd.DataFrame(index=institution_names,\n",
        "                                   columns=[\"Rating After \" + c.name for c in contest_list])\n",
        "\n",
        "        for contest in contest_list:\n",
        "            contest_name = contest.name\n",
        "            credit = credits_map.get(contest_name, 1.0)\n",
        "            actual_scores = {}\n",
        "            current_ratings = {}\n",
        "\n",
        "            for inst in institution_list:\n",
        "                perf = self.get_performance(inst, contest, contest_name)\n",
        "                if perf is not None:\n",
        "                    actual_scores[inst.name] = perf\n",
        "                    current_ratings[inst.name] = ratings[inst.name]\n",
        "                    marksheet.at[inst.name, contest_name + \" Point\"] = perf\n",
        "\n",
        "            if actual_scores:\n",
        "                min_score = min(actual_scores.values())\n",
        "                max_score = max(actual_scores.values())\n",
        "                if max_score > min_score:\n",
        "                    for name in actual_scores:\n",
        "                        actual_scores[name] = (actual_scores[name] - min_score) / (max_score - min_score)\n",
        "                else:\n",
        "                    for name in actual_scores:\n",
        "                        actual_scores[name] = 0.5\n",
        "\n",
        "            for name in actual_scores:\n",
        "                others = [r for n, r in current_ratings.items() if n != name]\n",
        "                expected = self.get_expected_performance(current_ratings[name], others)\n",
        "                actual = actual_scores[name]\n",
        "                delta = self.k * credit * (actual - expected)\n",
        "                ratings[name] += delta\n",
        "\n",
        "            for name in institution_names:\n",
        "                history_elo.at[name, \"Rating After \" + contest_name] = ratings[name]\n",
        "\n",
        "        marksheet.insert(0, \"Rating\", history_elo.iloc[:, -1])\n",
        "        marksheet = marksheet.infer_objects(copy=False)\n",
        "        history_elo = history_elo.infer_objects(copy=False)\n",
        "\n",
        "        marksheet.sort_values(by=\"Rating\", ascending=False, inplace=True)\n",
        "        history_elo = history_elo.loc[marksheet.index]\n",
        "\n",
        "        history_rank = history_elo.rank(axis=0, ascending=False, method='min')\n",
        "        history_rank.columns = [\"Rank After \" + c.name for c in contest_list]\n",
        "\n",
        "        marksheet = marksheet.round(0).astype('Int64')\n",
        "        history_elo = history_elo.round(0).astype('Int64')\n",
        "        history_rank = history_rank.astype('Int64')\n",
        "\n",
        "        contest_by_date = sorted(contest_list, key=lambda c: c.date, reverse=True)\n",
        "\n",
        "        sorted_point_cols = [c.name + \" Point\" for c in contest_by_date]\n",
        "        sorted_rating_cols = [\"Rating After \" + c.name for c in contest_by_date]\n",
        "        sorted_rank_cols = [\"Rank After \" + c.name for c in contest_by_date]\n",
        "\n",
        "        marksheet = marksheet[[\"Rating\"] + sorted_point_cols]\n",
        "        history_elo = history_elo[sorted_rating_cols]\n",
        "        history_rank = history_rank[sorted_rank_cols]\n",
        "\n",
        "        return marksheet, history_elo, history_rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AtCoderRatingCalculator:\n",
        "    def __init__(self, center=1500, decay=0.9, max_perf=None):\n",
        "        self.center = center\n",
        "        self.decay = decay\n",
        "        self.max_perf = max_perf\n",
        "\n",
        "    def get_performance(self, institution, contest, contest_name):\n",
        "        contest_teams = institution.get_contest_teams(contest_name)\n",
        "        if not contest_teams:\n",
        "            return None\n",
        "\n",
        "        best4 = sorted(contest_teams)[:4]\n",
        "        team_grades = []\n",
        "\n",
        "        rank_decay_rate = 0.02\n",
        "        for team in best4:\n",
        "            grade = 4000 * (1 - rank_decay_rate) ** (team.rank - 1) * (team.solved / contest.max_solved) ** 0.5\n",
        "            team_grades.append(grade)\n",
        "\n",
        "        k_norm = 3.14159\n",
        "        return (np.mean(np.array(team_grades) ** k_norm)) ** (1 / k_norm)\n",
        "\n",
        "    def compute_f_penalty(self, n):\n",
        "        if n == 0:\n",
        "            return 1200\n",
        "        numer = sum((0.81 ** i) for i in range(1, n + 1)) ** 0.5\n",
        "        denom = sum((self.decay ** i) for i in range(1, n + 1))\n",
        "        F_n = numer / denom\n",
        "\n",
        "        F_1 = (0.81 ** 1) ** 0.5 / (self.decay ** 1)\n",
        "        F_inf = (1 / (1 - 0.81)) ** 0.5 / (1 / (1 - self.decay))\n",
        "\n",
        "        return ((F_n - F_inf) / (F_1 - F_inf)) * 1200\n",
        "\n",
        "    def get_rating(self, institution_list, contest_list, credits_map):\n",
        "        contest_name_list = [c.name for c in reversed(contest_list)]\n",
        "        institution_names = [inst.name for inst in institution_list]\n",
        "\n",
        "        history = {name: [] for name in institution_names}\n",
        "        marksheet = pd.DataFrame(index=institution_names,\n",
        "                                 columns=[contest.name + \" Point\" for contest in contest_list])\n",
        "        history_rating = pd.DataFrame(index=institution_names,\n",
        "                                      columns=[\"Rating After \" + c.name for c in contest_list])\n",
        "\n",
        "        for idx, contest in enumerate(contest_list):\n",
        "            contest_name = contest.name\n",
        "            credit = credits_map.get(contest_name, 1.0)\n",
        "\n",
        "            for inst in institution_list:\n",
        "                perf = self.get_performance(inst, contest, contest_name)\n",
        "                if perf is None:\n",
        "                    continue\n",
        "\n",
        "                if self.max_perf is not None:\n",
        "                    perf = min(perf, self.max_perf)\n",
        "\n",
        "                history[inst.name].insert(0, (perf, credit))  # newest first\n",
        "                marksheet.at[inst.name, contest_name + \" Point\"] = perf\n",
        "\n",
        "                weighted_sum = 0\n",
        "                weight_total = 0\n",
        "                for i, (rperf, c) in enumerate(history[inst.name], 1):\n",
        "                    weight = c * (self.decay ** i)\n",
        "                    weighted_sum += rperf * weight\n",
        "                    weight_total += weight\n",
        "\n",
        "                if weight_total == 0:\n",
        "                    avg_perf = self.center\n",
        "                else:\n",
        "                    avg_perf = weighted_sum / weight_total\n",
        "\n",
        "                penalty = self.compute_f_penalty(len(history[inst.name]))\n",
        "                rating = avg_perf - penalty\n",
        "                history_rating.at[inst.name, \"Rating After \" + contest_name] = rating\n",
        "\n",
        "        marksheet.insert(0, \"Rating\", history_rating.iloc[:, -1])\n",
        "        marksheet = marksheet.infer_objects(copy=False)\n",
        "        history_rating = history_rating.infer_objects(copy=False)\n",
        "\n",
        "        marksheet.sort_values(by=\"Rating\", ascending=False, inplace=True)\n",
        "        history_rating = history_rating.loc[marksheet.index]\n",
        "\n",
        "        history_rank = history_rating.rank(axis=0, ascending=False, method='min')\n",
        "        history_rank.columns = [\"Rank After \" + c.name for c in contest_list]\n",
        "\n",
        "        marksheet = marksheet.round(0).astype('Int64')\n",
        "        history_rating = history_rating.round(0).astype('Int64')\n",
        "        history_rank = history_rank.astype('Int64')\n",
        "\n",
        "        contest_by_date = sorted(contest_list, key=lambda c: c.date, reverse=True)\n",
        "\n",
        "        sorted_point_cols = [c.name + \" Point\" for c in contest_by_date]\n",
        "        sorted_rating_cols = [\"Rating After \" + c.name for c in contest_by_date]\n",
        "        sorted_rank_cols = [\"Rank After \" + c.name for c in contest_by_date]\n",
        "\n",
        "        marksheet = marksheet[[\"Rating\"] + sorted_point_cols]\n",
        "        history_rating = history_rating[sorted_rating_cols]\n",
        "        history_rank = history_rank[sorted_rank_cols]\n",
        "\n",
        "        return marksheet, history_rating, history_rank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Perform Calculations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "RJD03aaQ-rCR",
        "outputId": "71ba4301-3a21-4291-8388-d2da6b69a55f"
      },
      "outputs": [],
      "source": [
        "calculator = GradeCalculator()\n",
        "marksheet, history_cgpa, history_rank = calculator.get_cgpa(institutions, contest_list, credits_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(marksheet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(history_cgpa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Export Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs('result', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"./result/contests.json\", \"w\") as file:\n",
        "    json.dump(json.loads(contest_list.__repr__()), file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"./result/institutions.json\", \"w\") as file:\n",
        "    json.dump(json.loads(institutions.__repr__()), file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "marksheet_df = marksheet.reset_index()\n",
        "marksheet_df.rename(columns={\"index\": \"Institution\"}, inplace=True)\n",
        "\n",
        "marksheet_df.to_csv(\"./result/marksheet.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history_cgpa_df = history_cgpa.reset_index()\n",
        "history_cgpa_df.rename(columns={\"index\": \"Institution\"}, inplace=True)\n",
        "\n",
        "history_cgpa_df.to_csv(\"./result/history_cgpa.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history_rank_df = history_rank.reset_index()\n",
        "history_rank_df.rename(columns={\"index\": \"Institution\"}, inplace=True)\n",
        "\n",
        "history_rank_df.to_csv(\"./result/history_rank.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
